{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare multiclass classification dataset\n",
    "\n",
    "def create_dataset():\n",
    "    #gerar um dataset 2d para classificação\n",
    "    X, y = make_blobs(n_samples = 1000, centers=20, n_features = 100, cluster_std=2,\n",
    "                     random_state = 2)\n",
    "    #one hot encode output\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    #dividir o dataset em treino e teste\n",
    "    n_train = 500\n",
    "    trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "    trainy, testy = y[:n_train, :], y[n_train:]\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treina o modelo de acordo com o número de camadas n_layers, retorna a acurrácia\n",
    "\n",
    "def evaluate_model(n_layers, trainX, trainy, testX, testy):\n",
    "    #configura a camada inicial de acordo com os dados\n",
    "    \n",
    "    n_input, n_classes = trainX.shape[1], testy.shape[1]\n",
    "    \n",
    "    #incia a definição do modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=n_input, activation='relu',\n",
    "                   kernel_initializer='he_uniform'))\n",
    "    for __ in range(1, n_layers):\n",
    "        model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.dd(Dense(n_classes, activation='sotmax'))\n",
    "    \n",
    "    #compila o modelo\n",
    "    opt = SGD(lr=0.01,momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    #fit model\n",
    "    history = model.fit(trainX, trainy, epoch=100, verbose = 0)\n",
    "    \n",
    "    #avalia o modelo no conjunto de testes\n",
    "    \n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    return history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avalia o modelo e plota a curva de aprendizagem para um numero de camadas\n",
    "\n",
    "all_history = list()\n",
    "num_layers = [1,2,3,4,,8,16,32]\n",
    "for n_layers in num_layers:\n",
    "    #avalia o modelo\n",
    "    \n",
    "    history, result = evaluate_model(n_layers, trainX,trainy, testX, testy)\n",
    "    print('layers=%d: %.3f' % (n_layers, result))\n",
    "    \n",
    "    #plota a curva\n",
    "    pyplot.plot(history.history['loss'], label=str(n_layers))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introdução: Vamos testar como a variação do número de camadas em uma rede neural pode impactar a capacidade de aprendizagem. Um modelo com 2 camadas escondidas com 5 nós cada não é igual a um modelo com 1 única camada e 10 neurônios nessa camada. O primeiro ter mais parâmetros e maior capacidade de aprendizagem. Mais parâmetros significa maior capacidade de memorização e mais facilidade em mapear amostras de treino e seus targets, contudo, perder poder generalização, causando overfiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1* Usarei o mesmo exemplo que fizemos a solução do aumento de neurônios, com um dataset criado através da função skleran.datasets.make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2* Depois criamos uma função para avaliar o modelo. Dessa vez precisamos fazer um loop para criar várias camadas de acordo com os parâmetros recebidos. Cada camada interna terá 10 neurônios, uma quantidade razóavel para aprender o mapeamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3* Passamos uma lista com o número de camadas como parâmetro. Observe que até 4 camadas os resultados melhoram, mas com muitas camadas os resultados começam a piorar e a ficarem \"inexplicáveis\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos o mesme teste com o dataset MNIST fashion, para detcção de objetos. A melhora nos resultados é quase imperceptível. Mas não ocorre o problema anterior por se tratar de um dataset um pouco mais complexo. O mesmo ocorre para a acurácia com o aumento das camadas. Tendo um resultado abaixo daquele que conseguimos ao aumentar o número de neurônios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conclusão é que aumentar o número de camadas(profundidade) de uma rede neural pode melhorar o resultados em datasets mais complexos. Em datasets simples demais o resultado pode ser impercptível ou até prejudicar a capacidade de modelo em generalizar; Somente com diversos testes podemos chegar no número ideal de camadas e neurônios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
